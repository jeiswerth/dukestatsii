---
title: "Homework 04"
author: "Jerry Eiswerth"
date: today
format: 
  html:
    self-contained: true
---

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(marginaleffects)
library(fixest)
```

# Part 1

## (1)

### (a)

$Y=2+3X+\epsilon$

$\leftrightarrow \epsilon=Y-2-3X$

$\leftrightarrow \epsilon=9-2-3*2=1$

### (b)

$\hat{Y}=1.9+3.1X+\epsilon$

$\leftrightarrow \hat{\epsilon}=\hat{Y}-1.9-3.1X$

$\leftrightarrow \hat{\epsilon}=9-1.9-3.1*2=0.9$

## (2)

Top Left: $C$ only causes $X$, $B$ only affects $Y$ through $X$, and $A$ does not cause $X$ except through $D$.

$Y=\beta_0 + \beta_1X + D + \epsilon$

Bottom Left: $D$ is a collider, so we don't include it. $Y=\beta_0 + \beta_1X + \epsilon$

Top Right: $A$ opens a backdoor path between $X$ and $Y$ and because it's unobserved, we cannot close it. We cannot identify the total effect of $X$ on $Y$. We can use $Y=\beta_0 + \beta_1X + \epsilon$, but the results will be biased.

Bottom Right: $Y = \beta_0 + \beta_1 X + \epsilon$

## (3)

### (a)

76.185 additional hours.

### (b)

19.693

### (c)

306.553 hours

### (d)

$AnnualHoursWorked=1256.671-238.853*2=778.965$

### (e)

Yes, the coefficient is statistically significant at the 95% level for both regressions. The coefficients are roughly 12-13 standard deviations from 0. This is the $t$ statistics.

Model 2: $t=\frac{-238.853}{19.693}=12.129$

Model 3: $t=\frac{-251.181}{19.28}=13.028$

## (4)

### (a)

$AnnualHoursWorked=\beta_0+\beta_1Edu+\beta_2Edu^2$

$\leftrightarrow \frac{\partial AnnualHoursWorked}{\partial Edu}=\frac{\partial}{\partial Edu}(\beta_0+\beta_1Edu+\beta_2Edu^2)$

$\leftrightarrow \frac{\partial AnnualHoursWorked}{\partial Edu}=\beta_1+2\beta_2Edu$

A one-year increase in Education predicts on average a $\beta_1+2\beta_2Edu$ increase in number of annual hours worked.

### (b)

$\frac{\partial AnnualHoursWorked}{\partial Edu}=\beta_1+2\beta_2Edu =110.230-2*1.581*16=59.638$

On average, the predict increase in annual hours worked is 59.638 for a one-year increase in Education, for an observation with 16 current years of Education

### (c)

$\beta_2$ is negative, and given the regression equations gives us the marginal effect: $\frac{\partial AnnualHoursWorked}{\partial Edu}=\beta_1+2\beta_2Edu$, annual hours worked will increase at a decreasing rate as education levels go up in general. However, if education were to increases to very high levels, we could see the marginal effect become 0 (around education = 35 years) and then turn negative - but it will still continue being less positive, and this level of education is unlikely as even PhD levels of education are not this high.

### (d)

Increasing the number of polynomial terms can lead to overfitting the data, which makes out of sample inference less useful. We also have trouble interpreting results with higher order polynomials.

## (5)

### (a)

If the family owns their home, then they are predicted to work 50.174 more hours on average than if they did not own their home. This is not statistically significant at the 95% level because the $t$ statistic is less than 1.96 - $t=\frac{50.174}{32.923}=1.52$

### (b)

$-923.904+773.412=-150.492$ Families with 4 children under the age of 5 are predicted to work on average 150 hours less than families with 3 children under the age of 5, holding home ownership constant.

### (c)

No, we cannot tell from the table alone. We would need more information from the regression in order to run a hypothesis test for the difference between the different types of families.

## (6)

### (a)

A one-unit increase in Education predicts a $110.073$ increase in annual hours worked, on average, for non homeowners and a $110.073-53.994=56.079$ increase for homeowners.

### (b)

The interaction term between Education and Homeowner is the difference in slopes for the regression lines for homeowners and non homeowners. This means that homeowners work 53.994 hours less annually than non homeowners for each year of additional education.

### (c)

For a one-unit increase in Education, model 2 predicts that annual hours worked will increase by 6.7% on average, holding all else constant.

### (d)

For a one percentage point increase in Education, annual hours worked will increase by $\frac{832.347}{100}=8.32$ on average, holding all else constant.

### (e)

The range of possible values for logged variables is restricted to values greater than zero. Thus, for any observations with 0 hours (or negative hours) worked, they would be excluded from the regression.

## (7)

\(b\) is the most accurate definition. Autocorrelation occurs when variables (including error terms) are correlated with lag or lead versions of themselves.

## (8)

\(b\) shows us how the variance of the errors changes with $X$, so we can see visually check for heteroskedasticity. (c) could also show heteroskedasticity like in (b), even though we use an omitted variable $Z$. All Linear Probability (binary OLS regression) models have heteroskedasticity by design, so (e) would tell us to definitely use heteroskedasticity-robust standard errors if the $Y$ is binary (if for some reason we did not already realize we were using an LPM).

## (9)

Weighted Least Squares can be used to offset the sampling bias. We compare the proportion of each type of demographic being included in the sample against the population value of those demographics, and assign a probability of being sampled. Then the inverse probability is used as a weight in the regression.

## (10)

\(a\) is non-classical measurement error because it is not random, in a way that is related to the true value. Certain individuals may decide not to answer truthfully, creating a pattern in misreporting.

# Part 2

## (1)

```{r}
d <- read_csv(here::here("homework_04_files", "dengue.csv"))

out <- lm(NoYes ~ humid + temp, 
          data = d)
summary(out)
```

## (2)

```{r}
out2 <- glm(NoYes ~ humid + temp,
            data = d,
            family = binomial(link = "logit"))
summary(out2)
```

```{r}
marg_eff <- avg_slopes(out2)
marg_eff
```

Humidity has a marginal effect of 0.03173 - holding temperature constant, a one-point increase in humidity increases the predicted probability of dengue being observed by 3.173 percentage points on average.

Temperature has a marginal effect of 0.00415 - holding humidity constant, a one-point increase in temperature increases the predicted probability of dengue being observed by 0.415 percentage points on average.

## (3)

```{r}
d2 <- na.omit(d[, c("humid", "temp")])

out3 <- lm(humid ~ temp,
           data = d2)
```

```{r}
d2$resid <- resid(out3)
d2$fitted <- fitted(out3)

p <- ggplot(data = d2,
            aes(x = fitted,
                y = resid)) +
  geom_point(alpha = 0.2)

p
```

We can see from the plot that the variance of the error term changes across the fitted values - it's small at low fitted values and increases at higher fitted values. It seems there is likely some level of heteroskedasticity.

```{r}
out4 <- feols(humid ~ temp,
              data = d2,
              se = 'hetero')

# Make the non-robust SE model in fixest to use in etable
out3 <- feols(humid ~ temp,
              data = d2)

etable(out3, out4)
```

There is clearly some level of heteroskedasticity as the robust standard errors are slightly higher, but it is not a dramatic difference - it does not meaningfully change the inferences we can make.

## (4)

```{r}
out5 <- feols(log(humid) ~ temp,
              data = d2,
              se = 'hetero')
summary(out5)
```

The model predicts a 5.65% increase in humidity on average from a 1 degree increase in temperature.

## (5)

From the plot in (3), the relationship does not look linear, and the pattern in the variance is changing with the residuals - and we can investigate further. If we look at the plot of humidity on temp, it clearly looks exponential.
```{r}
p2 <- ggplot(data = d2,
            aes(x = temp,
                y = humid)) +
  geom_point(alpha = 0.2)

p2
```

